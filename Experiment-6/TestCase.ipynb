{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff87f2c9-29a2-4e64-b046-2fef8bff1c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\CSE-DEPT\\anaconda3\\New folder\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSE-DEPT\\anaconda3\\New folder\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "C:\\Users\\CSE-DEPT\\anaconda3\\New folder\\Lib\\site-packages\\keras\\src\\layers\\layer.py:970: UserWarning: Layer 'crf' (of type CRF) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, TimeDistributed, Layer\n",
    "\n",
    "# ----------- Custom CRF Layer -----------\n",
    "class CRF(Layer):\n",
    "    def __init__(self, num_tags, **kwargs):\n",
    "        super(CRF, self).__init__(**kwargs)\n",
    "        self.num_tags = num_tags\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Transition matrix for CRF\n",
    "        self.transitions = self.add_weight(\n",
    "            shape=(self.num_tags, self.num_tags),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "            name=\"transitions\"\n",
    "        )\n",
    "        super(CRF, self).build(input_shape)\n",
    "\n",
    "    def call(self, logits):\n",
    "        return logits  # raw scores\n",
    "\n",
    "    def get_loss(self, y_true, y_pred):\n",
    "        return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "\n",
    "    def viterbi_decode(self, logits):\n",
    "        # Simplified greedy decode\n",
    "        return tf.argmax(logits, axis=-1)\n",
    "\n",
    "# ----------- Model Definition -----------\n",
    "max_len = 10      # keep it small for demo\n",
    "n_words = 5000    # vocab size\n",
    "n_tags = 7        # O, B-PER, I-PER, B-ORG, I-ORG, B-LOC, I-LOC\n",
    "\n",
    "inp = Input(shape=(max_len,))\n",
    "x = Embedding(input_dim=n_words, output_dim=50, input_length=max_len, mask_zero=True)(inp)\n",
    "x = Bidirectional(LSTM(units=50, return_sequences=True))(x)\n",
    "logits = TimeDistributed(Dense(n_tags))(x)\n",
    "\n",
    "crf = CRF(n_tags)\n",
    "out = crf(logits)\n",
    "\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "model.compile(optimizer=\"adam\", loss=crf.get_loss, metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d00d991-d9f2-427c-ac9a-7899a56ed2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSE-DEPT\\anaconda3\\New folder\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:857: UserWarning: Gradients do not exist for variables ['crf/transitions'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1edc1ffd040>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fake training data (2 examples only)\n",
    "X_train = np.array([\n",
    "    [10, 11, 12, 13, 0, 0, 0, 0, 0, 0],   # Elon Musk founded SpaceX\n",
    "    [14, 15, 16, 17, 0, 0, 0, 0, 0, 0]    # Google is in California\n",
    "])\n",
    "\n",
    "y_train = np.array([\n",
    "    [1, 2, 0, 3, 0, 0, 0, 0, 0, 0],  # B-PER I-PER O B-ORG\n",
    "    [3, 0, 0, 5, 0, 0, 0, 0, 0, 0]   # B-ORG O O B-LOC\n",
    "])\n",
    "\n",
    "# Train briefly\n",
    "model.fit(X_train, y_train, epochs=20, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6963876-ee7a-4434-80aa-1066e1dc0a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n",
      "Input Sentence                 Output Tags                   \n",
      "------------------------------------------------------------\n",
      "Elon Musk founded SpaceX       O O O O                       \n",
      "Google is in California        O O O O                       \n"
     ]
    }
   ],
   "source": [
    "# Mapping from indices to BIO tags\n",
    "idx2tag = {\n",
    "    0: \"O\",\n",
    "    1: \"B-PER\",\n",
    "    2: \"I-PER\",\n",
    "    3: \"B-ORG\",\n",
    "    4: \"I-ORG\",\n",
    "    5: \"B-LOC\",\n",
    "    6: \"I-LOC\"\n",
    "}\n",
    "\n",
    "# Input test sentences\n",
    "test_sentences = [\n",
    "    [\"Elon\", \"Musk\", \"founded\", \"SpaceX\"],\n",
    "    [\"Google\", \"is\", \"in\", \"California\"]\n",
    "]\n",
    "\n",
    "# Mock word2idx from training\n",
    "word2idx = {\"Elon\": 10, \"Musk\": 11, \"founded\": 12, \"SpaceX\": 13,\n",
    "            \"Google\": 14, \"is\": 15, \"in\": 16, \"California\": 17}\n",
    "\n",
    "# Convert to padded input\n",
    "X_test = []\n",
    "for sent in test_sentences:\n",
    "    seq = [word2idx.get(w, 1) for w in sent]\n",
    "    seq = seq + [0] * (max_len - len(seq))   # padding\n",
    "    X_test.append(seq)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Predict\n",
    "logits = model.predict(X_test)\n",
    "pred_ids = np.argmax(logits, axis=-1)\n",
    "\n",
    "# Show results\n",
    "print(f\"{'Input Sentence':30} {'Output Tags':30}\")\n",
    "print(\"-\" * 60)\n",
    "for sentence, pred in zip(test_sentences, pred_ids):\n",
    "    tags = [idx2tag[t] for t in pred[:len(sentence)]]\n",
    "    print(f\"{' '.join(sentence):30} {' '.join(tags):30}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d8af97-0a7e-4b9f-8766-ba087e64dc56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
